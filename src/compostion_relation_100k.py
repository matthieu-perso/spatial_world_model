# -*- coding: utf-8 -*-
"""compostion-relation-100k.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TYgbEZeGMYAEdRXBFzLIUDsclaX1mbzf
"""

import einops
import h5py
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
import re
import time
import torch

from dataclasses import dataclass
from google.colab import drive
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tabulate import tabulate
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
from typing import Dict, List, Tuple

"""## utils"""



# =================================
# function: extract representations
# =================================
def get_directions(linear_probe: torch.Tensor, options: int, mappings: Dict[str, int], verbose: bool=True) -> Dict[str, torch.Tensor]:
    """
    extract directions (unormalized) from linear probe
    """
    directions = linear_probe.detach().cpu().numpy()
    mapping = mappings["relation"]

    # create a mapping from class names to their weight vectors
    directions_dict = {}
    for relation, relation_index in mapping.items():
        if relation_index < options:
            directions_dict[relation] = directions[:, int(relation_index)]

    print(f"shape of direction vector (d_model,) = {directions_dict['above'].shape}" if verbose else "")

    return directions_dict

# ===================================
# function: print composition summary
# ===================================
def print_composition_summary(df: pd.DataFrame):
    print("\nðŸ“Š Compositional Relation Metrics (Original & PCA Spaces)")
    print(tabulate(df, headers="keys", tablefmt="github", floatfmt=".4f"))

    print("\nðŸ“ˆ Summary Statistics (Mean)")
    mean_df = df.select_dtypes(include=np.number).mean().to_frame().T
    print(tabulate(mean_df, headers="keys", tablefmt="github", floatfmt=".4f"))

    print("\nðŸ“ Compositional Pairs Sorted by PCA Angle")
    sorted_df = df.sort_values("Angle (PCA, Â°)", ascending=False)[
        ['Compositional Relation', 'Atomic Relation 1', 'Atomic Relation 2', 'Angle (PCA, Â°)']
    ]
    print(tabulate(sorted_df, headers="keys", tablefmt="github", floatfmt=".2f"))

# ======================================
# function: check compositional relation
# ======================================
def check_compositional_relations(directions: Dict[str, np.ndarray], verbose=True) -> pd.DataFrame:
    """
    Check compositional relations among spatial relation probes in original and PCA-reduced space.
    """
    composition_pairs = {
        "diagonally above and to the right of": ("above", "to the right of"),
        "diagonally above and to the left of": ("above", "to the left of"),
        "diagonally below and to the right of": ("below", "to the right of"),
        "diagonally below and to the left of": ("below", "to the left of"),
    }

    results = []

    for comp_rel, (atomic1, atomic2) in composition_pairs.items():
        if comp_rel in directions and atomic1 in directions and atomic2 in directions:
            direct_vec = directions[comp_rel]
            composed_vec = directions[atomic1] + directions[atomic2]

            cos_sim = cosine_similarity([direct_vec], [composed_vec])[0][0]
            euclidean_diff = np.linalg.norm(direct_vec - composed_vec)
            angle_deg = np.degrees(np.arccos(np.clip(cos_sim, -1.0, 1.0)))

            results.append({
                'Compositional Relation': comp_rel,
                'Atomic Relation 1': atomic1,
                'Atomic Relation 2': atomic2,
                'Cosine Similarity (Original)': cos_sim,
                'Euclidean Diff (Original)': euclidean_diff,
                'Angle (Original, Â°)': angle_deg
            })

            if verbose:
                print(f"\nðŸ”€ Original Space: '{comp_rel}' â‰ˆ '{atomic1}' + '{atomic2}'")
                print(f"  Cosine Similarity: {cos_sim:.4f}")
                print(f"  Euclidean Diff: {euclidean_diff:.4f}")
                print(f"  Angle: {angle_deg:.2f}Â°")

    # --- PCA projection ---
    relation_names = list(directions.keys())
    W = np.stack([directions[rel] for rel in relation_names])
    pca = PCA(n_components=2)
    W_2d = pca.fit_transform(W)
    directions_2d = {rel: vec for rel, vec in zip(relation_names, W_2d)}

    # General PCA plot
    fig, ax = plt.subplots(figsize=(8, 6))
    colors = plt.colormaps["tab10"]
    for i, rel in enumerate(relation_names):
        x, y = directions_2d[rel]
        ax.scatter(x, y, color=colors(i % 10), s=100, label=rel)
        ax.text(x + 0.03, y + 0.03, rel, fontsize=11)
    ax.set_title("PCA Projection of Spatial Relations")
    ax.set_xlabel("PC1")
    ax.set_ylabel("PC2")
    ax.grid(True)
    #ax.legend()
    plt.tight_layout()
    plt.show()

    # --- PCA compositional metrics & plots ---
    for idx, (comp_rel, (atomic1, atomic2)) in enumerate(composition_pairs.items()):
        if comp_rel in directions_2d and atomic1 in directions_2d and atomic2 in directions_2d:
            direct_vec_2d = directions_2d[comp_rel]
            composed_vec_2d = directions_2d[atomic1] + directions_2d[atomic2]

            cos_sim_pca = cosine_similarity([direct_vec_2d], [composed_vec_2d])[0][0]
            euclidean_diff_pca = np.linalg.norm(direct_vec_2d - composed_vec_2d)
            angle_deg_pca = np.degrees(np.arccos(np.clip(cos_sim_pca, -1.0, 1.0)))

            results[idx].update({
                'Cosine Similarity (PCA)': cos_sim_pca,
                'Euclidean Diff (PCA)': euclidean_diff_pca,
                'Angle (PCA, Â°)': angle_deg_pca
            })

            if verbose:
                print(f"\nðŸ§­ PCA Space: '{comp_rel}' â‰ˆ '{atomic1}' + '{atomic2}'")
                print(f"  Cosine Similarity: {cos_sim_pca:.4f}")
                print(f"  Euclidean Diff: {euclidean_diff_pca:.4f}")
                print(f"  Angle: {angle_deg_pca:.2f}Â°")

            # PCA composition arrows
            fig, ax = plt.subplots(figsize=(8, 6))
            origin = np.array([0, 0])
            ax.quiver(*origin, *directions_2d[atomic1], angles='xy', scale_units='xy', scale=1, color='blue', alpha=0.5, label=atomic1)
            ax.quiver(*origin, *directions_2d[atomic2], angles='xy', scale_units='xy', scale=1, color='green', alpha=0.5, label=atomic2)
            ax.quiver(*origin, *composed_vec_2d, angles='xy', scale_units='xy', scale=1, color='purple', alpha=0.7, label=f"{atomic1}+{atomic2}")
            ax.quiver(*origin, *direct_vec_2d, angles='xy', scale_units='xy', scale=1, color='red', alpha=0.7, label=comp_rel)

            ax.set_xlim(-15, 15)
            ax.set_ylim(-15, 15)
            ax.set_title(f"PCA Composition: {comp_rel}")
            ax.set_xlabel("PC1")
            ax.set_ylabel("PC2")
            ax.grid(True)
            ax.legend()
            plt.tight_layout()
            plt.show()

    # === Bonus Plot: Normalized PCA Vectors on Unit Circle (with Composed Sums) ===
    fig, ax = plt.subplots(figsize=(8, 8))  # Larger figure
    ax.set_title("Normalized PCA Vectors on Unit Circle")
    ax.set_xlabel("PC1")
    ax.set_ylabel("PC2")
    ax.set_aspect("equal")
    ax.grid(True)

    # Draw unit circle
    unit_circle = plt.Circle((0, 0), 1.0, color='lightgray', fill=False, linestyle='--')
    ax.add_artist(unit_circle)

    colors = plt.colormaps["tab10"]
    plotted_labels = set()

    # Build a color map to share colors between each triplet
    shared_color_map = {}
    for i, comp_rel in enumerate(composition_pairs.keys()):
        atomic1, atomic2 = composition_pairs[comp_rel]
        shared_color = colors(i % 10)
        shared_color_map[comp_rel] = shared_color
        shared_color_map[atomic1] = shared_color
        shared_color_map[atomic2] = shared_color

    # Plot atomic and direct vectors with shared colors
    for rel, vec in directions_2d.items():
        norm_vec = vec / np.linalg.norm(vec)
        color = shared_color_map.get(rel, 'gray')  # fallback for unrelated vectors
        ax.quiver(0, 0, norm_vec[0], norm_vec[1],
              angles='xy', scale_units='xy', scale=1,
              color=color, width=0.005, alpha=0.9,
              label=rel if rel not in plotted_labels else "")
        plotted_labels.add(rel)

    # Plot dashed normalized composition sums with matching color
    for comp_rel, (atomic1, atomic2) in composition_pairs.items():
        if all(r in directions_2d for r in [comp_rel, atomic1, atomic2]):
            vec1 = directions_2d[atomic1]
            vec2 = directions_2d[atomic2]
            composed_vec = vec1 + vec2
            composed_norm = composed_vec / np.linalg.norm(composed_vec)

            x_vals = [0, composed_norm[0]]
            y_vals = [0, composed_norm[1]]
            color = shared_color_map[comp_rel]
            label = f"{atomic1} + {atomic2} (normalized)"
            ax.plot(x_vals, y_vals, linestyle='--', linewidth=1.5,
                color=color, alpha=0.7,
                label=label if label not in plotted_labels else "")
            plotted_labels.add(label)

    ax.set_xlim(-1.2, 1.2)
    ax.set_ylim(-1.2, 1.2)
    ax.legend(fontsize=8, loc='center left', bbox_to_anchor=(1, 0.5))
    plt.tight_layout()
    plt.show()


    df = pd.DataFrame(results)

    if verbose and not df.empty:
        print_composition_summary(df)

    return df

"""## data utils"""

# =========================
# function: parse sentences
# =========================
def parse_sentences(
    sentences: List[str],
    verbose: bool = True,
    num_sample_triplets: int = 100
) -> Dict[str, List]:
    """
    Parses sentences into structured triplets with precise relation extraction.
    """
    start_time = time.time()

    triplets = []
    valid_indices = []

    # Updated regex pattern to capture the full multi-word relation
    pattern = r"The\s+(.*?)\s+is\s+(.*)\s+the\s+(.*?)\."

    for i, sentence in enumerate(sentences):
        # decode the byte string to a regular string
        sentence = sentence.decode('utf-8')
        match = re.match(pattern, sentence, re.IGNORECASE)
        if match:
            obj1 = match.group(1).strip()
            relation = match.group(2).strip()
            obj2 = match.group(3).strip()
            triplets.append((obj1, relation, obj2))
            valid_indices.append(i)

    if verbose:
        print("\n=== triplets ===")
        for i in range(min(num_sample_triplets, len(triplets))):
            print(f"{i}: {triplets[i]}")

    return {
        "triplets": triplets,
        "valid_indices": valid_indices
    }

# =========================
# function: encode triplets
# =========================
def encode_triplets(
    triplets: List[Tuple[str, str, str]],
    verbose: bool = True
) -> Tuple[Dict[str, np.ndarray], Dict[str, Dict[str, int]]]:
    """
    encode triplets for model training
    """
    start_time = time.time()
    print("encoding triplets..." if verbose else "")

    # extract separate components
    objects1, relations, objects2 = zip(*triplets)

    # filter out empty strings
    objects1 = [obj if obj else "UNKNOWN" for obj in objects1]
    relations = [rel if rel else "UNKNOWN" for rel in relations]
    objects2 = [obj if obj else "UNKNOWN" for obj in objects2]

    # encode each component
    obj1_encoder = LabelEncoder()
    rel_encoder = LabelEncoder()
    obj2_encoder = LabelEncoder()

    obj1_labels = obj1_encoder.fit_transform(objects1)
    rel_labels = rel_encoder.fit_transform(relations)
    obj2_labels = obj2_encoder.fit_transform(objects2)

    labels = {"object_1": obj1_labels, "relation": rel_labels, "object_2": obj2_labels}

    # create mapping dictionaries for later analysis
    obj1_mapping = dict(zip(obj1_encoder.classes_, range(len(obj1_encoder.classes_))))
    rel_mapping = dict(zip(rel_encoder.classes_, range(len(rel_encoder.classes_))))
    obj2_mapping = dict(zip(obj2_encoder.classes_, range(len(obj2_encoder.classes_))))

    mappings = {"object_1": obj1_mapping, "relation": rel_mapping, "object_2": obj2_mapping}

    if verbose:
        print(f"found {len(obj1_mapping)} unique objects as subject")
        print(f"found {len(rel_mapping)} unique relations")
        print(f"found {len(obj2_mapping)} unique objects as object")

        # Print some of the unique relations
        print("\nrelations used:")
        sample_relations = list(rel_mapping.keys())
        for i, rel in enumerate(sample_relations):
            print(f"{i}: {rel}")

        print(f"encoding completed in {time.time() - start_time:.2f} seconds")

    return labels, mappings

# ============================
# function: prepare data split
# ============================
def prepare_data_split(layer_data: torch.Tensor, valid_indices: List[int], labels: Dict[str, np.ndarray], test_size: float=0.2) -> Dict[str, torch.Tensor]:
    """
    prepare single train/test split for all labels
    """
    # filter embeddings to keep only valid indices
    X = layer_data[valid_indices]

    # create a single train/test split for all labels
    X_train, X_test, y_rel_train, y_rel_test = train_test_split(
        X,
        labels["relation"],
        test_size=test_size,
        random_state=42
        )

    # convert to PyTorch tensors
    X_train_tensor = X_train.clone().detach().float()
    X_test_tensor = X_test.clone().detach().float()

    y_rel_train_tensor = torch.tensor(y_rel_train, dtype=torch.long)
    y_rel_test_tensor = torch.tensor(y_rel_test, dtype=torch.long)

    return {
        'X_train': X_train_tensor,
        'X_test': X_test_tensor,
        'y_rel_train': y_rel_train_tensor,
        'y_rel_test': y_rel_test_tensor,
        }


# =============================
# function: create data loaders
# =============================
def create_data_loaders(split_data: Dict[str, torch.Tensor], batch_size: int=256) -> Dict[str, Dict[str, TensorDataset]]:
    """
    create data loaders for training and testing
    """
    # create training dataset
    train_rel_dataset = TensorDataset(split_data['X_train'], split_data['y_rel_train'])

    # create test dataset
    test_rel_dataset = TensorDataset(split_data['X_test'], split_data['y_rel_test'])

    # create data loaders
    train_rel_loader = DataLoader(train_rel_dataset, batch_size=batch_size, shuffle=True)
    test_rel_loader = DataLoader(test_rel_dataset, batch_size=batch_size)

    return {
        'train': {
            'relation': train_rel_loader,
            },
        'test': {
            'relation': test_rel_loader,
            }
        }

"""## linear probes"""

# ============================
# class: probe train arguments
# ============================
@dataclass
class ProbeTrainingArgs:
    verbose: bool = False
    device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # layer information
    layer_name: str = "layer_8"

    # spatial relation state
    options: int = 4
    d_model: int = 3072

    # standard training hyperparams
    epochs: int = 10

    # hyperparams for optimizer
    lr: float = 1e-3

    # cpde to get randomly initialized probe
    def setup_probe(self) -> torch.Tensor:
        linear_probe = torch.randn(self.d_model, self.options, device=self.device) / np.sqrt(self.d_model)
        linear_probe.requires_grad = True
        print(f"shape of linear probe is: d_model = {linear_probe.shape[0]} and options = {linear_probe.shape[-1]}" if self.verbose else "")
        return linear_probe

# ===========================
# class: Linear probe trainer
# ===========================
class LinearProbeTrainer:
    def __init__(self, args: ProbeTrainingArgs, dataloader: torch.utils.data.DataLoader):
        self.args = args
        self.linear_probe = args.setup_probe()
        self.dataloader = dataloader

    def train(self):
        if self.args.verbose:
            print(f"\ntraining a linear probe for spatial relations of layer {self.args.layer_name} for {self.args.epochs} epochs ...\n")
        self.step = 0

        # define optimizer
        optimizer = torch.optim.Adam(
            [self.linear_probe], lr=self.args.lr
        )

        # define loss criterion
        criterion = torch.nn.CrossEntropyLoss()

        for epoch in range(self.args.epochs):
            total_loss = 0
            correct = 0
            total = 0

            for inputs, targets in tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{self.args.epochs}"):

                # move data to device
                inputs, targets = inputs.to(self.args.device), targets.to(self.args.device)

                # get probe output
                probe_preds = einops.einsum(
                    inputs,
                    self.linear_probe,
                    "batch d_model, d_model options -> batch options",
                    )

                # compute loss
                loss = criterion(probe_preds, targets)

                # optimize
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

                # update loss and accuracy
                total_loss += loss.item()
                _, predicted = torch.max(probe_preds.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()

            # print epoch results
            avg_loss = total_loss / len(self.dataloader)
            accuracy = 100 * correct / total
            print(f"Epoch {epoch+1}/{self.args.epochs}: Loss={avg_loss:.4f}, Accuracy={accuracy:.2f}%")

# ========================
# function: evaluate probe
# ========================
def evaluate_probe(linear_probe: torch.Tensor, dataloader: DataLoader, device=torch.device("cuda" if torch.cuda.is_available() else "cpu")):
    """
    evaluate a linear probe model
    """
    all_preds = []
    all_targets = []

    with torch.inference_mode():
        for inputs, targets in dataloader:

            # move data to device
            inputs, targets = inputs.to(device), targets.to(device)

            # get linear probe preds
            probe_preds = einops.einsum(
                inputs,
                linear_probe,
                "batch d_model, d_model options -> batch options",
                )
            _, predicted = torch.max(probe_preds, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    accuracy = accuracy_score(all_targets, all_preds)
    report = classification_report(all_targets, all_preds, output_dict=True, zero_division=1)

    return accuracy, report

"""## main"""

# mount google drive
drive.mount('/content/drive')

# config variables
VERBOSE = True
selected_layer: int = 24

# define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"we are using {device}" if VERBOSE else "")

# seet rng seed
set_seed(1234)

# define the dataset name
dataset_path = "drive/MyDrive/datasets-sentences/simple_extended_101248_sentences.h5"

# load the data
dataset = h5py.File(dataset_path, 'r')

# list the datasets (for example, "sentences" and possibly an "embeddings" group)
print(list(dataset.keys()))

# define sentences and embeddings
sentences = dataset['sentences']
embeddings = dataset['embeddings']
layers_list = [8, 16, 24]

if VERBOSE:
    print("\n" + "=" * 40)
    print("ðŸ“Š dataset and embedding statistics")
    print("=" * 40)

    # dataset statistics
    print(f"\nðŸ”¢ dataset overview:")
    print(f"â€¢ total sentences: {len(sentences):,}")

    # layer details
    print(f"\nðŸ§¬ embedding layers extracted:")
    for k in layers_list:
        layer_name = f"layer_{k}"
        layer_shape = embeddings[layer_name].shape
        print(f"â€¢ layer {k}: shape: {layer_shape}")
    print("\n" + "=" * 40)

"""### training linear probes"""

# transform data to torch
layers_tensors = {}
for k in layers_list:
    layer_name = f"layer_{k}"
    layers_tensors[layer_name] = torch.tensor(embeddings[layer_name][:])
    print(f"layer {k} embeddings shape (# sentences, d_model): {layers_tensors[layer_name].shape}" if VERBOSE else "")

# parse senteces
pars_sentences = parse_sentences(sentences, verbose=VERBOSE, num_sample_triplets=20)

# encode triplets
labels, mappings = encode_triplets(pars_sentences["triplets"], verbose=VERBOSE)

# prepare layer data for split
batch_size = {}
dataloaders = {}
test_size = 0.2

# get number of relations
options = len(mappings["relation"])
print(f"number of unique relations: {options}" if VERBOSE else "")

for k in layers_list:
    layer_name = f"layer_{k}"
    batch_size[layer_name], d_model = layers_tensors[layer_name].shape
    print(f"layer data shape (num, d_model): {layers_tensors[layer_name].shape}" if VERBOSE else "")

    # create dataloaders
    split_data = prepare_data_split(layers_tensors[layer_name], pars_sentences["valid_indices"], labels, test_size=test_size)
    dataloaders[layer_name] = create_data_loaders(split_data, batch_size=1024)

"""### train linear probes"""

# train the linear probe
torch.set_grad_enabled(True)
args = {}
trainers = {}

for k in layers_list:
    layer_name = f"layer_{k}"
    args[layer_name] = ProbeTrainingArgs(
        d_model=d_model,
        options=options,
        layer_name=layer_name,
        epochs=100,
        lr=1e-3,
        verbose=VERBOSE
        )

    trainers[layer_name] = LinearProbeTrainer(args[layer_name], dataloader=dataloaders[layer_name]["train"]["relation"])      # we focus only on the spatial relation
    trainers[layer_name].train()

"""### evaluate linear probes"""

# report accuracy
accuracy = {}
report = {}

for k in layers_list:
    layer_name = f"layer_{k}"
    accuracy[layer_name], report[layer_name] = evaluate_probe(trainers[layer_name].linear_probe, dataloaders[layer_name]["test"]["relation"], device=device)
    print(f"Linear probe accuracy: {accuracy[layer_name]:.4f}" if VERBOSE else "")

# extract the directions of each spatial relation as a Dict
directions = {}

for k in layers_list:
    layer_name = f"layer_{k}"
    directions[layer_name] = get_directions(trainers[layer_name].linear_probe, options, mappings, verbose=VERBOSE)  # each direction has shape (d_model,)

# check directions' properties
directions_df = {}

for k in layers_list:
    layer_name = f"layer_{k}"
    print(f"\ninverse relation analysis for layer {k}...")
    directions_df[layer_name] = check_compositional_relations(directions[layer_name])

